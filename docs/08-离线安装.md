## 离线安装

### 约定

- 本文以 CentOS 7.8 操作系统为例进行讲解。

### 准备离线包

- 点击这里获取[离线包](https://github.com/TimeBye/kubeadm-ha/actions?query=workflow%3AOffline)
  - 下载 kubeadm-ha-1.20.1.tar
  - 下载 docker-ce-19.03.13.tar.gz

  ![](https://i.loli.net/2020/12/22/fcQeuvXxjqk6o1a.png)
### 离线安装 kubernetes 集群

#### 节点信息

|    **ip**     | **hostname** |   **OS**   |      **role**      |
| :-----------: | :----------: | :--------: | :----------------: |
| 192.168.56.11 |    node1     | CentOS 7.8 | master etcd worker |
| 192.168.56.12 |    node2     | CentOS 7.8 | master etcd worker |
| 192.168.56.13 |    node3     | CentOS 7.8 | master etcd worker |
| 192.168.56.14 |    node4     | CentOS 7.8 |       worker       |
| 192.168.56.15 |    deploy    | CentOS 7.8 |      ansible       |

**注意：** 以下操作未特殊说明都在 `192.168.56.15` deploy 节点执行。

#### ansible 环境准备

- 上传准备好的 2 个缓存文件至搭建 ansible 环境的服务器上，目录为 /root。

- 准备 ansible 环境
  ```shell
  # 解压 docker-ce-19.03.13.tar.gz
  mkdir docker-ce
  tar -xzvf docker-ce-19.03.13.tar.gz -C docker-ce
  
  # 添加本地源
  cat <<EOF > /etc/yum.repos.d/docker-ce.repo
  [docker-ce-stable]
  name=Docker CE Stable - \$basearch
  baseurl=file:///root/docker-ce/
  enabled=1
  gpgcheck=0
  repo_gpgcheck=0
  EOF

  # 安装 docker
  yum install -y \
    lvm2 \
    device-mapper-persistent-data
  yum install -y \
    docker-ce-19.03.13 \
    docker-ce-cli-19.03.13 \
    containerd.io-1.3.7
  
  # 启动 docker
  systemctl start docker
  
  # 加载 docker 镜像
  docker load -i kubeadm-ha-1.20.1.tar
  ```

- 运行 kubeadm-ha 镜像
  ```shell
  docker run -d --restart=always --name ansible -p 12480:80 \
    -v $PWD/my-cluster:/etc/ansible/my-cluster \
    -v $PWD/cluster-backup:/etc/ansible/cluster-backup \
    setzero/kubeadm-ha:1.20.1
  ```

#### 编写配置文件

- 编辑变量文件 `./my-cluster/variables.yaml`，内容如下
  ```yaml
  # 设置为离线模式
  install_mode: offline
  # 所有 yum 源地址都配置为 ansible 镜像运行的服务器的IP，注意地址末尾 / 必须加上
  base_yum_repo: http://192.168.56.15:12480/yum/
  epel_yum_repo: http://192.168.56.15:12480/yum/
  docker_yum_repo: http://192.168.56.15:12480/yum/
  kubernetes_yum_repo: http://192.168.56.15:12480/yum/
  ```

- 升级内核配置（若准备离线包时升级了内核，则必须配置，否则跳过）
    <details>

    - 编辑 `./my-cluster/variables.yaml` 追加以下字段
      ```yaml
      # 若需升级内核添加一下变量，不升级则不添加
      kernel_centos:
      - http://192.168.56.15:12480/yum/kernel-ml-4.20.13-1.el7.elrepo.x86_64.rpm
      - http://192.168.56.15:12480/yum/kernel-ml-devel-4.20.13-1.el7.elrepo.x86_64.rpm
      ```
    </details>

- 配置 ansible inventory 文件 `./my-cluster/inventory.ini`，内容如下
  ```ini
  ; 将所有节点的信息在这里填写
  ;    第一个字段                  为节点内网IP，部署完成后为 kubernetes 节点 nodeName
  ;    第二个字段 ansible_port     为节点 sshd 监听端口
  ;    第三个字段 ansible_user     为节点远程登录用户名
  ;    第四个字段 ansible_ssh_pass 为节点远程登录用户密码
  [all]
  192.168.56.11 ansible_port=22 ansible_user="vagrant" ansible_ssh_pass="vagrant"
  192.168.56.12 ansible_port=22 ansible_user="vagrant" ansible_ssh_pass="vagrant"
  192.168.56.13 ansible_port=22 ansible_user="vagrant" ansible_ssh_pass="vagrant"
  192.168.56.14 ansible_port=22 ansible_user="vagrant" ansible_ssh_pass="vagrant"
      
  ; 私有云：
  ;    VIP 负载模式：
  ;       也就是负载均衡器 + keepalived 模式，比如常用的 haproxy + keepalived。
  ;       本脚本中负载均衡器有 nginx、openresty、haproxy、envoy 可供选择，设置 lb_mode 即可进行任意切换。
  ;       设置 lb_kube_apiserver_ip 即表示启用 keepalived，请先与服务器提供部门协商保留一个IP作为 lb_kube_apiserver_ip，
  ;       一般 lb 节点组中有两个节点就够了，lb节点组中第一个节点为 keepalived 的 master 节点，剩下的都为 backed 节点。
  ;
  ;    节点本地负载模式：
  ;       只启动负载均衡器，不启用 keepalived（即不设置 lb_kube_apiserver_ip），
  ;       此时 kubelet 链接 apiserver 地址为 127.0.0.1:lb_kube_apiserver_port。
  ;       使用此模式时请将 lb 节点组置空。
  ;
  ; 公有云：
  ;    不推荐使用 slb 模式，建议直接使用节点本地负载模式。
  ;    若使用 slb 模式，请先使用节点本地负载模式进行部署，
  ;    部署成功后再切换至 slb 模式：
  ;       将 lb_mode 修改为 slb，将 lb_kube_apiserver_ip 设置为购买到的 slb 内网ip，
  ;       修改 lb_kube_apiserver_port 为 slb 监听端口。
  ;    再次运行初始化集群脚本即可切换至 slb 模式。
  [lb]
      
  ; 注意etcd集群必须是1,3,5,7...奇数个节点
  [etcd]
  192.168.56.11
  192.168.56.12
  192.168.56.13
      
  [kube-master]
  192.168.56.11
  192.168.56.12
  192.168.56.13
      
  [kube-worker]
  192.168.56.11
  192.168.56.12
  192.168.56.13
  192.168.56.14
      
  ; 预留组，后续添加master节点使用
  [new-master]
      
  ; 预留组，后续添加worker节点使用
  [new-worker]
      
  ; 预留组，后续添加etcd节点使用
  [new-etcd]

  ; 预留组，后续删除master角色使用
  [del-master]

  ; 预留组，后续删除etcd角色使用
  [del-etcd]

  ; 预留组，后续删除节点使用
  [del-node]
  
  ;-------------------------------------- 以下为基础信息配置 ------------------------------------;
  [all:vars]
  ; 是否跳过节点物理资源校验，Master节点要求2c2g以上，Worker节点要求2c4g以上
  skip_verify_node=false
  ; 容器运行时类型，可选项：containerd，docker；默认 containerd
  container_manager="containerd"
  ; kubernetes版本
  kube_version="1.20.1"
  ; 负载均衡器
  ;   有 nginx、haproxy、envoy 和 slb 四个选项，默认使用 nginx；
  lb_mode="nginx"
  ; 使用负载均衡后集群 apiserver ip，设置 lb_kube_apiserver_ip 变量，则启用负载均衡器 + keepalived
  ; lb_kube_apiserver_ip="192.168.56.15"
  ; 使用负载均衡后集群 apiserver port
  lb_kube_apiserver_port="8443"
      
  ; 网段选择：pod 和 service 的网段不能与服务器网段重叠，
  ; 若有重叠请配置 `kube_pod_subnet` 和 `kube_service_subnet` 变量设置 pod 和 service 的网段，示例参考：
  ;    如果服务器网段为：10.0.0.1/8
  ;       pod 网段可设置为：192.168.0.0/18
  ;       service 网段可设置为 192.168.64.0/18
  ;    如果服务器网段为：172.16.0.1/12
  ;       pod 网段可设置为：10.244.0.0/18
  ;       service 网段可设置为 10.244.64.0/18
  ;    如果服务器网段为：192.168.0.1/16
  ;       pod 网段可设置为：10.244.0.0/18
  ;       service 网段可设置为 10.244.64.0/18
  ; 集群pod ip段
  kube_pod_subnet="10.244.0.0/18"
  ; 集群service ip段
  kube_service_subnet="10.244.64.0/18"
      
  ; 集群网络插件，目前支持flannel,calico
  network_plugin="calico"
      
  ; 若服务器磁盘分为系统盘与数据盘，请修改以下路径至数据盘自定义的目录。
  ; Kubelet 根目录
  kubelet_root_dir="/var/lib/kubelet"
  ; docker容器存储目录
  docker_storage_dir="/var/lib/docker"
  ; containerd容器存储目录
  containerd_storage_dir="/var/lib/containerd"
  ; Etcd 数据根目录
  etcd_data_dir="/var/lib/etcd"
  ```

- 执行安装
  - 升级内核（若准备离线包时升级了内核，则必须执行，否则跳过）
    <details>

    ```
    docker exec -it ansible \
      ansible-playbook -i my-cluster/inventory.ini -e @my-cluster/variables.yaml 00-kernel.yml
    ```
    </details>

  - 不升级内核
    ```
    docker exec -it ansible \
      ansible-playbook -i my-cluster/inventory.ini -e @my-cluster/variables.yaml 90-init-cluster.yml
    ```

### 安装 helm

**注意：** 以下操作未特殊说明都在 `192.168.56.11` 第一台 master 节点执行。

1. 下载helm客户端

    ```bash
    curl -sLo helm-v3.4.2-linux-amd64.tar.gz http://192.168.56.15:12480/helm-v3.4.2-linux-amd64.tar.gz
    ```

2. 解压压缩包（以linux-amd64为例）

    ```bash
    tar -zxvf helm-v3.4.2-linux-amd64.tar.gz
    ```

3. 将文件移动到PATH目录中（以linux-amd64为例）

    ```bash
    sudo mv linux-amd64/helm /usr/bin/helm
    ```

#### 验证部署

- 执行命令，出现以下信息即部署成功。
    
    ```console
    $ helm version
    version.BuildInfo{Version:"v3.4.2", GitCommit:"23dd3af5e19a02d4f4baa5b2f242645a1a3af629", GitTreeState:"clean", GoVersion:"go1.14.13"}
    ```